% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tool_groq.R
\name{query_groq}
\alias{query_groq}
\title{Query the Groq API with a Prompt}
\usage{
query_groq(
  prompt,
  config,
  model = c("llama-3.1-8b-instant", "llama3-8b-8192", "llama3-70b-8192",
    "deepseek-r1-distill-llama-70b"),
  temperature = 1,
  top_p = 1,
  max_tokens = 1024,
  stream = FALSE
)
}
\arguments{
\item{prompt}{A character string containing the prompt text for the model.}

\item{config}{A named list with Groq configuration parameters:
\describe{
\item{\code{api_key}}{Your Groq API key (as a string).}
\item{\code{url}}{Groq API endpoint (e.g., \code{"https://api.groq.com/openai/v1/chat/completions"}).}
\item{\code{model}}{Model name to use, such as \code{"llama3-70b-8192"}.}
}}
}
\value{
A character string with the model's generated response.
}
\description{
Sends a prompt to the Groq-hosted LLM (e.g., llama3-70b-8192) using the OpenAI-compatible API format.
This function assumes a standard completion interface similar to OpenAIâ€™s chat completion endpoint.
}
\examples{
config <- tool_set_config("groq")
query_groq("Write a haiku about LLMs.", config)

}
